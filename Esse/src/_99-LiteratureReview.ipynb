{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from all_imports import *\n",
    "import requests\n",
    "import csv\n",
    "import _modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "\n",
    "# find first, all studies regarind outdoor thermal comfort\n",
    "# check the primary pattern\n",
    "# second run, parse thorugh the filtered list of studies and get the metadata\n",
    "# filter the data by needing to have all valid data, exepct number of votes or thermal history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Configuration\n",
    "API_KEY = _modules.SCOPUS_API_KEY  # Replace with your Scopus API key\n",
    "INST_TOKEN = _modules.INST_TOKEN   # Replace with your institutional token\n",
    "SEARCH_QUERY = 'TITLE-ABS-KEY(\"outdoor thermal comfort study\") AND PUBYEAR AFT 1999 AND PUBYEAR BEF 2025'\n",
    "BASE_URL = 'https://api.elsevier.com/content/search/scopus'\n",
    "CSV_FILENAME = 'scopus_outdoor_thermal_comfort.csv'\n",
    "\n",
    "# Define regex patterns for extracting locations\n",
    "location_patterns = [\n",
    "    r'\\b(?:located in|in the region of|in the city of|conducted in|performed in|studied in)\\s+([A-Z][a-zA-Z]+(?:\\s[A-Z][a-zA-Z]+)*)',\n",
    "    r'\\b(?:country of|nation of|state of)\\s+([A-Z][a-zA-Z]+(?:\\s[A-Z][a-zA-Z]+)*)',\n",
    "]\n",
    "\n",
    "# Define regex patterns for extracting number of participants and votes\n",
    "participants_pattern = r'\\b(?:number of participants|sample size|participants? count|sample size is)\\s*[:\\-]?\\s*(\\d+)'\n",
    "votes_pattern = r'\\b(?:number of votes|votes count|total votes|voted by)\\s*[:\\-]?\\s*(\\d+)'\n",
    "\n",
    "# Enhanced primary regex pattern to identify studies on outdoor thermal comfort with participants\n",
    "primary_pattern = r'(?=.*\\bstudy\\b)(?=.*\\boutdoor\\s+comfort\\b)(?=.*\\bparticipants?\\b)'\n",
    "\n",
    "# Define secondary regex patterns for additional keywords related to outdoor thermal comfort\n",
    "secondary_patterns = [\n",
    "    r'\\bthermal\\s+comfort\\b',\n",
    "    r'\\boutdoor\\s+(environment|settings|conditions|spaces|areas)\\b',\n",
    "    r'\\bmicroclimate\\b',\n",
    "    r'\\burban\\s+heat\\s+island\\b',\n",
    "    r'\\bheat\\s+stress\\b',\n",
    "    r'\\bthermal\\s+perception\\b',\n",
    "    r'\\bphysiological\\s+responses?\\b',\n",
    "    r'\\bcomfort\\s+models?\\b',\n",
    "    r'\\beffect\\s+of\\s+.+\\bon\\s+outdoor\\s+comfort\\b',\n",
    "    r'\\bassessment\\s+of\\s+thermal\\s+comfort\\s+in\\b',\n",
    "    r'\\bfield\\s+measurements?\\b',\n",
    "    r'\\bsimulation(s)?\\b',\n",
    "    r'\\bquestionnaire(s)?\\b',\n",
    "    r'\\bresearch\\s+(aim|objective|purpose|methodology|findings|results)\\b',\n",
    "]\n",
    "\n",
    "# Function to extract city, country, number of participants, and number of votes using regex patterns\n",
    "def extract_details(text):\n",
    "    found_locations = []\n",
    "    matched_sentences = []\n",
    "    number_of_participants = None\n",
    "    number_of_votes = None\n",
    "    text_references = []\n",
    "    \n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split text into sentences\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for pattern in location_patterns:\n",
    "            matches = re.findall(pattern, sentence)\n",
    "            if matches:\n",
    "                found_locations.extend(matches)\n",
    "                matched_sentences.append(sentence)\n",
    "        \n",
    "        # Check for number of participants\n",
    "        participants_match = re.search(participants_pattern, sentence, re.IGNORECASE)\n",
    "        if participants_match:\n",
    "            number_of_participants = participants_match.group(1)\n",
    "            text_references.append(f\"Participants data found: {sentence}\")\n",
    "\n",
    "        # Check for number of votes\n",
    "        votes_match = re.search(votes_pattern, sentence, re.IGNORECASE)\n",
    "        if votes_match:\n",
    "            number_of_votes = votes_match.group(1)\n",
    "            text_references.append(f\"Votes data found: {sentence}\")\n",
    "\n",
    "    # Remove duplicate locations\n",
    "    found_locations = list(set(found_locations))\n",
    "    \n",
    "    return found_locations, number_of_participants, number_of_votes, matched_sentences, text_references\n",
    "\n",
    "# Function to determine if text indicates a study on outdoor thermal comfort with participants\n",
    "def is_outdoor_thermal_comfort_study(text):\n",
    "    if re.search(primary_pattern, text, re.IGNORECASE):\n",
    "        for pattern in secondary_patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to fetch full-text document from Elsevier API\n",
    "def fetch_full_text(doi):\n",
    "    url = f'https://api.elsevier.com/content/article/doi/{doi}?view=FULL'\n",
    "    headers = {\n",
    "        'X-ELS-APIKey': API_KEY,\n",
    "        'X-ELS-Insttoken': INST_TOKEN,\n",
    "        'Accept': 'text/plain'  # Change to 'text/xml' for XML format\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.text, False  # Return False for 'Error' column if no error\n",
    "    else:\n",
    "        print(f\"Error fetching full text for DOI {doi}: {response.status_code}, {response.text}\")\n",
    "        return None, True  # Return True for 'Error' column if error\n",
    "\n",
    "# Function to fetch metadata from Scopus API\n",
    "def fetch_metadata(api_key, inst_token, query, max_papers=10):\n",
    "    headers = {\n",
    "        'X-ELS-APIKey': api_key,\n",
    "        'X-ELS-Insttoken': inst_token,\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'query': query,\n",
    "        'view': 'COMPLETE',\n",
    "        'count': max_papers\n",
    "    }\n",
    "\n",
    "    papers = []\n",
    "    start_index = 0\n",
    "    \n",
    "    while len(papers) < max_papers:\n",
    "        params['start'] = start_index\n",
    "        response = requests.get(BASE_URL, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching data: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        if 'search-results' not in data or 'entry' not in data['search-results']:\n",
    "            break\n",
    "\n",
    "        entries = data['search-results']['entry']\n",
    "        if not entries:\n",
    "            break\n",
    "\n",
    "        for entry in entries:\n",
    "            if len(papers) >= max_papers:\n",
    "                break\n",
    "            \n",
    "            doi = entry.get('prism:doi', 'N/A')\n",
    "            title = entry.get('dc:title', 'N/A')\n",
    "            authors = entry.get('dc:creator', 'N/A')\n",
    "            publication_date = entry.get('prism:coverDate', 'N/A')\n",
    "            volume = entry.get('prism:volume', 'N/A')\n",
    "            issue = entry.get('prism:issueIdentifier', 'N/A')\n",
    "            keywords = entry.get('authkeywords', 'N/A')\n",
    "            document_type = entry.get('subtypeDescription', 'N/A')\n",
    "\n",
    "            # Fetch full text for the article\n",
    "            full_text, error = fetch_full_text(doi)\n",
    "            if full_text:\n",
    "                # Determine if the full text indicates an outdoor thermal comfort study with participants\n",
    "                if is_outdoor_thermal_comfort_study(full_text):\n",
    "                    # Extract locations, number of participants, votes, matched sentences, and text references from full text\n",
    "                    cities, num_participants, num_votes, matched_sentences, text_references = extract_details(full_text)\n",
    "                else:\n",
    "                    continue  # Skip if the text doesn't match the thermal comfort criteria\n",
    "            else:\n",
    "                cities, num_participants, num_votes, matched_sentences, text_references = [], None, None, [], []\n",
    "\n",
    "            if isinstance(keywords, list):\n",
    "                keywords = '; '.join(keywords)\n",
    "\n",
    "            papers.append({\n",
    "                'DOI': doi,\n",
    "                'Title': title,\n",
    "                'Authors': authors,\n",
    "                'Publication Date': publication_date,\n",
    "                'Volume': volume,\n",
    "                'Issue': issue,\n",
    "                'Keywords': keywords,\n",
    "                'Document Type': document_type,\n",
    "                'City': '; '.join(cities) if cities else 'N/A',\n",
    "                'Country': '; '.join(cities) if cities else 'N/A',\n",
    "                'Number of Participants': num_participants if num_participants else 'N/A',\n",
    "                'Number of Votes': num_votes if num_votes else 'N/A',\n",
    "                'Matched Sentences': ' | '.join(matched_sentences) if matched_sentences else 'N/A',\n",
    "                'Text Reference': ' | '.join(text_references) if text_references else 'N/A',\n",
    "                'Error': 'True' if error else ''  # Add 'Error' column data\n",
    "            })\n",
    "\n",
    "        start_index += len(entries)\n",
    "        \n",
    "    return papers\n",
    "\n",
    "# Function to save papers to CSV\n",
    "def save_to_csv(filename, papers):\n",
    "    fieldnames = ['DOI', 'Title', 'Authors', 'Publication Date', 'Volume', 'Issue', 'Keywords', 'Document Type', 'City', 'Country', 'Number of Participants', 'Number of Votes', 'Matched Sentences', 'Text Reference', 'Error']\n",
    "    \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for paper in papers:\n",
    "            writer.writerow(paper)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    papers = fetch_metadata(API_KEY, INST_TOKEN, SEARCH_QUERY, max_papers=5)  # Limit to 10 papers\n",
    "    save_to_csv(CSV_FILENAME, papers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
